dqn:
  lrate: 0.001
  discount_factor: 0.99
  polyak_factor: 0.01
  n_step: 1
  epsilon_annealing_steps: 2000
  epsilon_min: 0.01
  memory_capacity: 2000
  batch_size: 32
  grad_norm_value: 10.0
  warm_up_steps: 1000
  update_frequency: 1
  v_min: -100.0
  v_max: 100.0
  atoms_dim: 51
  noise_type: 'factorized'
  use_soft_update: True
  use_double_q: True
  use_dueling: True
  use_categorical: True
  use_combined_experience_replay: True
  use_reward_normalization: True
  use_state_normalization: False
  reward_norm_clip: [-5.0, 5.0]
  state_norm_clip: [-5.0, 5.0]
  logdir: "./output/dqn"
  seed: 1
qrdqn:
  lrate: 0.001
  discount_factor: 0.99
  polyak_factor: 0.01
  n_step: 1
  epsilon_annealing_steps: 2000
  epsilon_min: 0.01
  memory_capacity: 2000
  batch_size: 64
  grad_norm_value: 0.0
  quantiles_dim: 200
  warm_up_steps: 1000
  update_frequency: 1
  noise_type: 'factorized'
  use_soft_update: True
  use_double_q: False
  use_combined_experience_replay: True
  use_reward_normalization: False
  use_state_normalization: False
  reward_norm_clip: [-5.0, 5.0]
  state_norm_clip: [-5.0, 5.0]
  logdir: "./output/qrdqn"
  seed: 1
ddpg:
  actor_lrate: 0.0001
  critic_lrate: 0.001
  l2_reg_value: 0.001
  discount_factor: 0.99
  polyak_factor: 0.005
  n_step: 1
  action_bounds: [-1.0, 1.0]
  noise_type: "ou"
  mean: 0.0
  sigma: 0.2
  sigma_min: 0.0
  n_step_annealing: 1000000
  theta: 0.15
  dt: 0.01
  memory_capacity: 1000000
  batch_size: 64
  warm_up_steps: 10000
  update_frequency: 1
  use_combined_experience_replay: True
  use_reward_normalization: False
  use_state_normalization: False
  reward_norm_clip: [-5.0, 5.0]
  state_norm_clip: [-5.0, 5.0]
  logdir: "./output/ddpg"
  seed: 1
td3:
  actor_lrate: 0.001
  critic_lrate: 0.001
  discount_factor: 0.99
  polyak_factor: 0.005
  n_step: 1
  action_limit: [-1.0, 1.0]
  target_noise_limit: [-0.5, 0.5]
  noise_std: 0.1
  target_noise_std: 0.2
  policy_update_frequency: 2
  memory_capacity: 1000000
  batch_size: 100
  warm_up_steps: 10000
  update_frequency: 1
  use_combined_experience_replay: True
  use_reward_normalization: False
  use_state_normalization: False
  reward_norm_clip: [-5.0, 5.0]
  state_norm_clip: [-5.0, 5.0]
  logdir: "./output/td3"
  seed: 1
sac:
  policy_lrate: 0.0003
  q_values_lrate: 0.0003
  alpha_lrate: 0.0003
  discount_factor: 0.99
  polyak_factor: 0.005
  n_step: 1
  memory_capacity: 1000000
  batch_size: 256
  warm_up_steps: 10000
  update_frequency: 1
  use_combined_experience_replay: True
  use_reward_normalization: False
  use_state_normalization: False
  reward_norm_clip: [-5.0, 5.0]
  state_norm_clip: [-5.0, 5.0]
  logdir: "./output/sac"
  seed: 1
